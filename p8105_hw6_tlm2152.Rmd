---
title: "P8105 Homework 6"
author: "Tucker Morgan (tlm2152)"
date: "12/4/2021"
output: github_document
---

```{r setup, message = FALSE}
library(tidyverse)
library(modelr)
library(patchwork)
```

## Problem 1

```{r loading birthweight data}
birthweight_df <- read_csv("./Data/birthweight.csv")
str(birthweight_df)

na_check <- birthweight_df[rowSums(is.na(birthweight_df)) > 0,]
na_check
rm(na_check)
```

Looks like I will want to convert a handful of variables like `babysex`, `frace`, `malform`, and `mrace` to factors instead of numeric vectors. And it doesn't seem like there are any NA values to account for before analysis.

```{r cleaning birthweight data}
birthweight_df <- 
  birthweight_df %>% 
  mutate(babysex = factor(babysex,
                          levels = c(1, 2),
                          labels = c("male", "female"))) %>% 
  mutate(frace = factor(frace,
                        levels = c(1, 2, 3, 4, 8, 9),
                        labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown"))) %>% 
  mutate(malform = factor(malform,
                          levels = c(0, 1),
                          labels = c("abset", "present"))) %>% 
  mutate(mrace = factor(mrace,
                        levels = c(1, 2, 3, 4, 8),
                        labels = c("White", "Black", "Asian", "Puerto Rican", "Other")))
```

My proposed regression model for birthweight is based on hypothesized factors. I hypothesize that the following factors will affect birthweight:

  * `gaweeks`: gestational age in weeks - hypothesized to increase weight with greater age;
  * `malform`: presence of malformations that could affect weight - hypothesized to decrease weight;
  * `momage`: mother's age at delivery (years) - hypothesized to decrease weight with greater mother age;
  * `wtgain`: mother's weight gain during pregnancy (pounds) - hypothesized to increase birthweight with greater weight gain during pregnancy.

```{r birthweight models}
hyp_mod = lm(bwt ~ gaweeks + malform + momage + wtgain, data = birthweight_df)
main_fx_mod = lm(bwt ~ blength + gaweeks, data = birthweight_df)
interact_mod = lm(bwt ~ bhead + blength + babysex + (bhead * blength) + (bhead * babysex) + (bhead * blength * babysex), data = birthweight_df)

hyp_mod %>% broom::tidy()
```

There are significant p-values here for some of my hypothesized predictors. Particularly `gaweeks` and `wtgain` seem to have the most statistically significant effects. Let's look at some residuals for the hypothesized model.

```{r residuals predictions and plotting}
birthweight_df %>% 
  add_predictions(hyp_mod) %>% 
  add_residuals(hyp_mod) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(x = "Predicted Birthweight (grams)", y = "Residual", title = "Hypothesized Model Predictions vs Residuals")
```

Most of the residuals seem to be fairly evenly dispersed about 0, however there are some rather large differences. For instance some predicted birthweights of 2000 grams have a residual of over 1000 grams. We also seem to have more positive residuals on the low end of our predictions and more negative residuals towards the maximum prediction, which might be concerning for this model. Let's take a look at a couple of others and compare root mean squared error (RMSE).

```{r main effects and interactions models}
cv_df <- 
  crossv_mc(birthweight_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) %>% 
  mutate(
    hyp_model      = map(.x = train, ~lm(bwt ~ gaweeks + malform + momage + wtgain, data = .x)),
    main_fx_model  = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    interact_model = map(.x = train, ~lm(bwt ~ bhead + blength + babysex + (bhead * blength) + (bhead * babysex) + (bhead * blength * babysex), data = .x))
    ) %>% 
  mutate(
    rmse_hyp      = map2_dbl(.x = hyp_model, .y = test, ~rmse(model = .x, data = .y)),
    rmse_main_fx  = map2_dbl(.x = main_fx_model, .y = test, ~rmse(model = .x, data = .y)),
    rmse_interact = map2_dbl(.x = interact_model, .y = test, ~rmse(model = .x, data = .y))
    )
```

```{r plotting RMSE for each model}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

Based on the plot above, it looks like the hypothesized model has a much higher RMSE compared to the other two. This lends credence to the idea of using model diagnostics and data-driven model-building processes rather than using hypotheses alone.

## Problem 2

Now, let's look at bootstrapping with weather data from `rnoaa`.
```{r loading NOAA data}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Next, I'll use `bootstrap` to take 5000 samples with replacement of the same size as the original data set.
```{r bootstrapping}
set.seed(23)

boot_weather <- bootstrap(weather_df, 5000) %>% 
  rename(boot_sample = strap, boot_id = .id) %>% 
  mutate(boot_id = as.integer(boot_id))
```

```{r checking bootstraps}
as.data.frame(boot_weather$boot_sample[[1]])
```

And it looks like this has the correct number of rows and columns that we would expect. Next, I'll calculate $\hat{r}^2$ or `r_squared` and $\log(\beta_0 * \beta_1)$ or `log_var` for each bootstrap sample.

```{r bootstrap analysis}
boot_results <- 
  boot_weather %>% 
  mutate(
    models = map(boot_sample, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy),
    glance = map(models, broom::glance)
  ) %>% 
  select(-boot_sample, -models) %>% 
  unnest(c(results, glance), names_repair = "unique") %>% 
  group_by(boot_id)

boot_clean <- 
  boot_results %>% 
  select(boot_id, term, estimate, r.squared, adj.r.squared) %>% 
  pivot_wider(names_from = term, values_from = estimate) %>% 
  janitor::clean_names() %>% 
  mutate(log_var = log(intercept * tmin))
```

```{r bootstrap plots}
r_sq_dist <- 
  boot_clean %>% 
  ggplot(aes(x = r_squared, fill = "brickred")) +
  geom_density(alpha = 0.2) +
  labs(title = "Distribution of r_squared") +
  theme(legend.position = "none") +
  stat_function(fun = dnorm,
                args = with(boot_clean, c(mean = mean(r_squared), sd = sd(r_squared))),
                color = "red",
                size = 1.5,
                alpha = .7)
  
log_var_dist <- 
  boot_clean %>% 
  ggplot(aes(x = log_var)) +
  geom_density(alpha = 0.2, fill = "dodgerblue1") +
  labs(title = "Distribution of log_var") +
  stat_function(fun = dnorm,
                args = with(boot_clean, c(mean = mean(log_var), sd = sd(log_var))),
                color = "blue",
                size = 1.5,
                alpha = .7)
  
r_sq_dist / log_var_dist
```

And when looking at the estimate distributions, they both look fairly normal. I've included a normal distribution with the respective means and standard deviations for comparison. One could argue that the distributions are slightly left-skewed, but they are very close to normal.

```{r quantiles}
r_squared_bounds <- 
  quantile(pull(boot_clean, r_squared), probs = c(.025, 0.975), names = FALSE, type = 4)

log_var_bounds <- 
  quantile(pull(boot_clean, log_var), probs = c(.025, 0.975), names = FALSE, type = 4)

data.frame(estimate = c("r_squared", "log_var"),
          lower_bound = c(r_squared_bounds[[1]], log_var_bounds[[1]]),
          upper_bound = c(r_squared_bounds[[2]], log_var_bounds[[2]]))
```

So the bounds of `r_squared` are `r round(r_squared_bounds[[1]], digits = 2)` and `r round(r_squared_bounds[[2]], digits = 2)`. And the bounds of `log_var` are `r round(log_var_bounds[[1]], digits = 2)` and `r round(log_var_bounds[[2]], digits = 2)`. Now let's look at the same distribution plots with these bounds shown.

```{r plots with quantiles}
r_sq_dist <- 
  boot_clean %>% 
  ggplot(aes(x = r_squared, fill = "brickred")) +
  geom_density(alpha = 0.2) +
  labs(title = "Distribution of r_squared", y = "density") +
  theme(legend.position = "none") +
  stat_function(fun = dnorm,
                args = with(boot_clean, c(mean = mean(r_squared), sd = sd(r_squared))),
                color = "red",
                size = 1.5,
                alpha = .7) +
  geom_vline(xintercept = c(r_squared_bounds),
             linetype = "dashed")
  
log_var_dist <- 
  boot_clean %>% 
  ggplot(aes(x = log_var)) +
  geom_density(alpha = 0.2, fill = "dodgerblue1") +
  labs(title = "Distribution of log_var", y = "density") +
  stat_function(fun = dnorm,
                args = with(boot_clean, c(mean = mean(log_var), sd = sd(log_var))),
                color = "blue",
                size = 1.5,
                alpha = .7) +
    geom_vline(xintercept = c(log_var_bounds),
             linetype = "dashed")
  
r_sq_dist / log_var_dist
```

That's nice.
